{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skadina/VTI-Data/blob/main/notebooks/NLPPipeSimple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Pipe\n",
        "### Vienkāršots piemērs NLP Pipe rīka pielietošanai\n",
        "Šajā lapā"
      ],
      "metadata": {
        "id": "91yXuDh0Ro5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQSDORfGRg6Y"
      },
      "outputs": [],
      "source": [
        "# Augšuplādējam google colab vidē teksta failu\n",
        "!wget https://raw.githubusercontent.com/LUMII-AILab/NLP_Course/main/notebooks/resources/velnini.txt\n",
        "# filename = \"velnini.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importējam kodam nepieciešamās bibliotēkas/palīgrīkus\n",
        "import requests\n",
        "import urllib\n",
        "import json"
      ],
      "metadata": {
        "id": "43dnnqhiSHxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nolasām teksta failā esošo tekstu\n",
        "file = open(filename, 'r', encoding=\"utf-8\")\n",
        "text = file.read()\n",
        "\n",
        "# Lai pārbaudītu, ka teksts ir pareizi nolasīts, izdrukājam mainīgā \"text\" saturu\n",
        "print(text)"
      ],
      "metadata": {
        "id": "m10ctywgSoTi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP-Pipe izsaukšana\n",
        "Šī ir galvenā funkcija, kura nosūta tekstu NLP-Pipe rīkam un nolasa saņemto atbildi."
      ],
      "metadata": {
        "id": "vWDJ6lHXUYIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nodefinējam funkciju, kas izsauc LV-PIPE REST API\n",
        "def process_lvpipe(text, steps=None, api_url=\"https://nlp.ailab.lv/api/nlp\"):\n",
        "#    steps = steps or ['tokenizer', 'morpho', 'parser'] # +ner\n",
        "    steps = steps or ['tokenizer', 'morpho'] # 'parser',  +ner\n",
        "\n",
        "    response = requests.post(api_url, json={'data': {'text': text}, 'steps': steps})\n",
        "    return response.json()['data']"
      ],
      "metadata": {
        "id": "kO09_a7oT8PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iepriekš izveidotajai funkcijai ir iespējams padot tekstu, kuru nepieciešams analizēt."
      ],
      "metadata": {
        "id": "THrpjpZXUjfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iepriekš no faila nolasīto tekstu analizējam ar NLP-Pipe rīku.\n",
        "doc = process_lvpipe(text)\n",
        "\n",
        "# Rakstām visu NLP-Pipe rīka atgriezto informāciju failā.\n",
        "with open(\"lvpipe_output.json\", 'w') as f:\n",
        "    json.dump(doc, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "# Apskatām kādu informāciju ir atgriezis NLP-Pipe rīks – izprintēsim šī dokumeneta saturu.\n",
        "with open(\"lvpipe_output.json\", 'r') as f:\n",
        "    print(f.read())\n",
        "\n",
        "# Rezultātu var arī apskatīt failā \"lvpipe_output.json\", kam vajadzētu atrasties pie google colab failiem.\n",
        "# Ja neparādās, tad var nospiest pogu \"refresh\", lai atjauninātu mapes saturu."
      ],
      "metadata": {
        "id": "uReLzxdGUfXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lai gan iegūtajā rezultātā jau ir visa analizētā informācija, JSON formāts nav labi cilvēkam pārskatāms.<br>\n",
        "Tāpēc pievienoju arī papildus kodu, kas pārveido šo informāciju uz CoNLL formātu. <br>\n",
        "Šis kods ir apslēpts, tādēļ ka ir diezgan garš, bet principā viss ko tas dara ir pārkārto informāciju no vienas pieraksta formas uz otru."
      ],
      "metadata": {
        "id": "7JBQLko5V-2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pārveidošana no JSON uz CoNLL {\"display-mode\":\"form\"}\n",
        "# Most of the work in NLP is to convert data from one format to another ;)\n",
        "\n",
        "# This code block allows for customisation and format conversion,\n",
        "# since other NLP pipelines might use different data formats and naming conventions.\n",
        "\n",
        "# ID     = \"index\"\n",
        "FORM   = \"form\"\n",
        "LEMMA  = \"lemma\"\n",
        "# UPOS   = \"upos\"\n",
        "XPOS   = \"tag\"\n",
        "# FEATS  = \"ufeats\"\n",
        "# HEAD   = \"parent\"\n",
        "# DEPREL = \"deprel\"\n",
        "# DEPS   = \"deps\"\n",
        "# MISC   = \"misc\"\n",
        "\n",
        "# FORMAT = [ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC]\n",
        "FORMAT = [FORM, LEMMA, UPOS, XPOS]\n",
        "\n",
        "\n",
        "# For each token, creates a TAB-separated list of its UD features\n",
        "def token_features(token):\n",
        "    line = \"\"\n",
        "    cols = FORMAT\n",
        "    for x in cols:\n",
        "        if token.get(x): line += str(token[x]) + '\\t'\n",
        "#        else: line += \"_\\t\"\n",
        "    return line.lstrip('\\t') + '\\n'\n",
        "\n",
        "\n",
        "# De-tokenizes a tokenized sentence\n",
        "def detokenize_sentence(sentence):\n",
        "    text = \"\"\n",
        "    for token in sentence[\"tokens\"]:\n",
        "        text += token[FORM] + ' ' # Naive glueing\n",
        "    return text.lstrip(' ')\n",
        "\n",
        "\n",
        "# Reads a JSON file produced by LV-PIPE and converts it to the CoNLL-U format\n",
        "def json_to_conll(json_file, conll_file):\n",
        "    with open(json_file, encoding=\"utf-8\") as j_file:\n",
        "        data = json.load(j_file)\n",
        "\n",
        "    with open(conll_file, 'w', encoding=\"utf-8\") as c_file:\n",
        "        for sentence in data[\"sentences\"]:\n",
        "            c_file.write(\"# text = {}\\n\".format(detokenize_sentence(sentence)))\n",
        "\n",
        "            for token in sentence[\"tokens\"]:\n",
        "                c_file.write(token_features(token))\n",
        "\n",
        "            c_file.write('\\n')"
      ],
      "metadata": {
        "id": "OSy5ye7iWedQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CoNLL faila izprintēšana {\"display-mode\":\"form\"}\n",
        "# Pretty-prints a CoNLL-U file\n",
        "def print_conllu(conll_file):\n",
        "    with open(conll_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    structured_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"#\"):\n",
        "            # Comment lines\n",
        "            structured_lines.append(line.strip())\n",
        "        elif line.strip():\n",
        "            # Non-empty non-comment lines\n",
        "            structured_lines.append(line.strip().split(\"\\t\"))\n",
        "        else:\n",
        "            # Empty lines - sentence breaks\n",
        "            structured_lines.append('')\n",
        "\n",
        "    # Find the maximum width of each column\n",
        "    data_rows = [row for row in structured_lines if isinstance(row, list)]\n",
        "    max_widths = [max(len(row[i]) for row in data_rows) for i in range(len(data_rows[0]))]\n",
        "\n",
        "    for line in structured_lines:\n",
        "        if isinstance(line, list):\n",
        "            # A data row\n",
        "            print(' '.join(word.ljust(max_widths[i]) for i, word in enumerate(line)))\n",
        "        else:\n",
        "            # A comment or empty line\n",
        "            print(line)\n",
        "            if line == \"\": print()"
      ],
      "metadata": {
        "id": "NOtRjcGKXjEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Izsaucam pārveidošanu no JSON formāta uz CoNLL formātu\n",
        "json_to_conll(\"lvpipe_output.json\", \"lvpipe_output.conll\")\n",
        "\n",
        "# Lai pārbaudītu, ka teksts ir pareizi analizēts, izprintējam CoNLL faila saturu\n",
        "print_conllu(\"lvpipe_output.conll\")\n",
        "\n",
        "# Rezultātu var arī apskatīt failā \"lvpipe_output.conll\", kam vajadzētu atrasties pie google colab failiem.\n",
        "# Ja neparādās, tad var nospiest pogu \"refresh\", lai atjauninātu mapes saturu."
      ],
      "metadata": {
        "id": "VZmeJF2-XkJm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}